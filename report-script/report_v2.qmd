---
format:
  html:
    toc: true
    toc-float: true
code-fold: true
---

# Performance and Sustainability Analysis of open-source LLM

## Overview

The rapid development of Large Language Models (LLMs) has created a need to understand and compare their properties beyond benchmark performance alone. This project consolidates data from the Hugging Face API and leaderboards to enable a holistic view of the LLM landscape. By combining these dimensions, we aim to contribute to a more transparent, data-driven understanding of modern LLMs.

The project pursues both analytical and methodological objectives. From a research perspective, it explores and investigates relationships between benchmark scores and model size, and between user engagement and CO₂ consumption. From a learning perspective, it develops a complete data science workflow: collecting data through APIs and web scraping, merging and cleaning datasets, as well as visualizing and statistically modeling the results.

```{python}
#| label: imports
from pathlib import Path
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import LinearSegmentedColormap
from matplotlib import patches
import seaborn as sns
from matplotlib.ticker import PercentFormatter
import os
from plotnine import ggplot, aes, geom_point, geom_smooth, labs, theme_bw, scale_x_log10
import plotly.express as px
import plotly.graph_objects as go
from scipy.stats import ttest_ind
import statsmodels.formula.api as smf
from IPython.display import HTML
```

## Methods

::: columns
::: {.column style="padding-right: 1rem;"}
### API

To retrieve data from the Hugging Face API, we began by exploring the API documentation and analyzing its multi-layered JSON responses to identify key metadata fields such as author, license, model type, downloads, likes, and timestamps.

The final implementation uses a Python class (`HuggingFaceAPI`) that loads an authentication token from a `.env` file for secure API access. The script retrieves the most downloaded models for the text-generation pipeline, collects comprehensive metadata, and applies regular expressions to extract model sizes (e.g., "7B", "13B") from model names. The processed data is saved as `huggingface_llm_metadata.csv`, providing a foundational dataset to be enriched with benchmark performance data from web scraping.
:::

::: {.column style="padding-right: 1rem;"}
### Scraping

The HuggingFace Open Leaderboard presents benchmark data in a dynamic table within an iframe, displaying only 28 models by default. Since automated scrolling proved unreliable, we adopted URL-based filtering via `?search=[model_name]` parameters to access specific models.

The implementation uses an `HFLeaderboardScraper` class built with Selenium and BeautifulSoup, supporting multiple browsers. The scraper navigates the DOM hierarchy—locating the iframe and extract data from standard HTML table elements. It incorporates multiple strategies, validation to handle absent models, duplicate entries, as well as of potential stale element issues.

The resulting dataset is incrementally written and shares a common identifier (`Model`/`modelId`) with the API-collected metadata, enabling straightforward merging for analysis.
:::
:::

**Note**: Scraping Hugging Face was permitted, as no robots.txt file restricted access.

### Data preparation and cleaning

The cleaning process merged API and leaderboard data into a unified table (`df_joined`).

Duplicate rows and missing values were checked. Model identifiers were normalized through a custom cleaning function to ensure that both datasets could be easily joined. Missing model_size values were retrieved from Hugging Face model cards via a *short code scraper*, with one remaining value for `DialoGPT-medium` added manually.

Dates were converted to `datetime`, percentage and CO₂ values were parsed into numeric format, and the `model_size` variable was expressed in billions for consistency. Symbolic `type icons` were mapped to categorical labels. Data ranges were validated for plausibility. Outliers were identified using the IQR method; however, the values were retained as they reflected genuinely large models.

Finally, two new variables were added: `score per billion parameters` (for performance efficiency) and `score per kg CO₂` (for environmental efficiency). The resulting dataset was saved as **df_joined.pkl** for further analysis.

## Q 1 - Which models offer the best size-to-performance ratio?

```{python}
#| label: import

df_joined = pd.read_pickle(Path("../data", "final", "df_joined_clean.pkl"))

#colors
sns.set_theme(style="ticks", palette="pastel", font_scale=1.1)
COLORS = {
    "main": "#A1C9F4",   # blue
    "accent": "#FFB482", # pastel orange
    "cat3": "#8DE5A1"    # green
}
```

To explore which models offer the best size-to-performance ratio, we examined the relationship between model parameters (in billions) and average benchmark scores using a LOESS smoother. We keep the x-axis linear to maximize readability: absolute size differences and performance plateaus are immediately visible.

```{python}
#| label: avg_benchmark_vs_size_loess
#| warning: false
#| fig-cap: "Figure 1: Average benchmark score vs. model size with LOESS smoother"
p=(
    ggplot(df_joined, aes(x="model_size", y="average"))
    + geom_point(alpha=0.30, color=COLORS["main"])
    + geom_smooth(method="loess", span=0.9, color=COLORS["accent"], se=False)
    + labs(
    title="Average Score vs Model Size (LOESS smoother)",
    x="Model Size (B)",
    y="Average Score (%)"
    )
    + theme_bw()
)
p
```

The LOESS curve reveals non-linear scaling: performance rises rapidly for small to mid-sized models and plateaus beyond approximately 20–30B parameters, indicating **diminishing performance gains at larger scales.** The Pearson correlation coefficient captures the strong overall trend, though it does not fully represent the non-linear relationship.

```{python}
#| label: corr_model_size_perf
#| output: asis

corr = df_joined[["model_size", "average"]].corr(method="pearson").iloc[0,1]
print(f"The resulting coefficient (r ≈ {corr:.2f}) confirms that larger models achieve higher benchmark scores.")
```

While larger models dominate benchmark rankings, notable exceptions exist: *Qwen 2.5-32B-Instruct* achieves the highest average score (≈46.6%) despite being half the size of *Meta-Llama 3 70B* models. This demonstrates that model architecture, training efficiency, and fine-tuning strategies can outweigh sheer scale. Other compact models such as *Phi-4* and *Llama 3-8B-Instruct* achieve strong scores for their size, highlighting that smaller, well-optimized models can offer efficient size-to-performance trade-offs.

::: {.callout-note collapse="true" title="Top models by sizw with benchmark performance metrics"}
```{python}
#| label: table_model_size_vs_benchmarks
#| tbl-cap: "Top models by size with benchmark performance metrics"
#| tbl-cap-location: top

benchmark_cols = [
    "model", "model_size", "average", "score_per_billion"]
available_cols = [c for c in benchmark_cols if c in df_joined.columns]

tbl_bench = (
    df_joined[available_cols]
    .sort_values("average", ascending=False)
    .head(10)
    .copy()
)

num_cols = [c for c in tbl_bench.columns if c not in ["model"]]
tbl_bench[num_cols] = tbl_bench[num_cols].round(2)

tbl_bench
```
:::

## Q2 - Which models are the most cost-effective?

We examine the **environmental costs** of models during their lifecycle and inference, using *CO₂ cost* as a proxy for sustainability. Rather than relying solely on benchmark metrics, we use **user likes** as a holistic measure of perceived usefulness, integrating aspects like accuracy, usability, documentation, and community engagement.

To compare models’ popularity and acceptance, we normalize likes by downloads, yielding likes per download - a metric expressing how well a model is received relative to its adoption.

```{python}
#| label: Dataprep

df_joined["likes_per_download"] = df_joined["likes"] / df_joined["downloads"].replace(0, pd.NA)
```

We visualize the joint distribution of CO₂ cost and likes per download using a bivariate kernel density estimate (KDE) with overlaid data points. This continuous representation avoids binning artifacts and reveals data structure effectively given our small sample size. Moreover, the plot includes a "Balanced Best Zone" (green shaded area), defined as models in the lowest 50% of CO₂ costs and highest 50% of likes per download, indicating optimal balance between sustainability and user appreciation.

```{python}
#| label: density_plot_likes_per_download_vs_co2_cost
#| fig-cap: "Figure 2: Density distribution 'Likes / Download' vs. 'CO2 costs'"

blue = "#A1C9F4"

white_to_blue = LinearSegmentedColormap.from_list(
    "white_to_blue",
    [
        (0.0, "#F8FBFE"),
        (0.1, "#D6E7FA"),
        (0.2, "#B4D3F7"),
        (1.0, "#A1C9F4"),
    ]
)

plt.figure(figsize=(8, 5))
sns.kdeplot(
    x=df_joined["co₂ cost"],
    y=df_joined["likes_per_download"],
    fill=True,
    cmap=white_to_blue,
    alpha=0.6,
    levels=20,
    thresh=0,
    cut=1
)
plt.xlim(0, 80)
sns.scatterplot(
    x=df_joined["co₂ cost"],
    y=df_joined["likes_per_download"],
    color="black",
    s=25,
    alpha=0.6
)

plt.text(2, 0.0108, "mistralai/mistral-7b-v0.1", fontsize=9, weight="bold", color="#4CAF50")
plt.text(3, 0.0061, "meta-llama/llama-2-7b-chat-hf", fontsize=9, weight="bold", color="black")
plt.text(52, 0.0037, "meta-llama/llama-3.3-70b-instruct", fontsize=9, weight="bold", color="#FFB482")

ax = plt.gca()
zone = patches.Rectangle(
    (df_joined["co₂ cost"].min(), df_joined["likes_per_download"].quantile(0.5)),
    df_joined["co₂ cost"].quantile(0.5) - df_joined["co₂ cost"].min(),
    ax.get_ylim()[1] - df_joined["likes_per_download"].quantile(0.5),
    linewidth=1.8,
    edgecolor="#4CAF50",
    facecolor="#4CAF50",
    alpha=0.25,
    label="Balanced best zone"
)

ax.add_patch(zone)

plt.xlabel("CO2-Cost")
plt.ylabel("Likes / Download")
plt.title("Density distribution 'Likes / Download' vs. 'CO2 costs'")
plt.legend(loc="upper right")
plt.tight_layout()
for spine in plt.gca().spines.values():
    spine.set_linewidth(0.8)
plt.show()
```

As, we can see, most models concentrate within a narrow, low CO₂ cost range, suggesting that frequently downloaded models tend to be computationally efficient - likely because users operate with limited resources and large models may not run on their hardware. A few large-scale outliers with substantially higher emissions indicate that scaling still incurs significant environmental costs. Approximately half the models achieve modest likes per download, while a smaller subset shows much higher user approval. Models in the *green Balanced Best Zone* combine low ecological footprint with high user value, demonstrating that **strong perceived quality and sustainability are not mutually exclusive**. This subset points to promising directions for future model optimization.

## Q 3 - How do open-source models differ from proprietary ones in terms of performance?

### Average benchmark score by access type comparison

::: columns
::: {.column width="36%"}
To examine whether model accessibility influences performance, we compared average benchmark scores of open-source and proprietary LLMs. A practical performance threshold was introduced as a visual reference. Each dot represents a model, color-coded by access type. The dashed gray line marks the 25% threshold.

While proprietary models more frequently exceed this level, several open-source models approach or surpass it, suggesting that performance parity is increasingly attainable.
:::

::: {.column width="4%"}
<!-- empty spacer column -->
:::

::: {.column width="60%"}
```{python}
#| label: avg_score_threshold_plotly
#| #| fig-responsive: false

df_joined_plot = df_joined.dropna(subset=["average", "access type", "model_size"]).copy()

color_map = {
    "open-source": "#A1C9F4",
    "proprietary": "#8DE5A1"
}

fig = px.strip(
    df_joined_plot,
    x="access type",
    y="average",
    color="access type",
    color_discrete_map=color_map,
    hover_name="model",
    hover_data={
        "average": True,
        "model_size": True,
        "access type": False
    },
    title="Score by Access Type (interactive)"
)

fig.add_hline(
    y=25,
    line_dash="dash",
    line_color="gray",
    line_width=2,
    annotation_text="25% Practical Threshold",
    annotation_position="top left"
)

fig.update_traces(
    jitter=0.3,
    opacity=0.85,
    marker=dict(size=9, line=dict(width=0))
)

fig.update_layout(
    template="simple_white",
    font=dict(size=13),
    title_x=0.05,
    yaxis_title="Average Score (%)",
    xaxis_title="Access Type",
    height=400,
    width=450,
    autosize=False,
    hoverlabel=dict(bgcolor="white", font_size=12),
    showlegend=False
)

fig.update_xaxes(showline=True, linewidth=1, linecolor="black", mirror=True)
fig.update_yaxes(showline=True, linewidth=1, linecolor="black", mirror=True)

fig.show()
```
Figure 3: Average benchmark score by access type.
:::
:::

::: {.callout-note collapse="true" title="Community Convention"}
Following the [Hugging Face Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) convention, we define *practical accuracy* as achieving an average benchmark score above *25%*-the approximate level at which models demonstrate consistent reasoning and instruction-following capabilities. This threshold serves as an informative benchmark, not a strict pass/fail cutoff.
:::

To assess whether these performance differences are statistically significant, we conducted a *Welch's t-test* on average benchmark scores. This test does not assume equal variances or sample sizes—appropriate given that proprietary models are fewer but sometimes larger. The result (t = -1.76, p = 0.09) indicates no statistically significant difference between groups.

To control for model size differences, we performed an *ANCOVA* with `average` score as the dependent variable, `log10(model_size)` as a covariate, and `access type` as a categorical predictor. The ANCOVA results show that model size significantly predicts performance: every ten-fold increase in parameters yields approximately a 13-point rise in average benchmark score. Crucially, access type (open-source vs. proprietary) shows no measurable effect when controlling for size. Model scale, not licensing status, is the primary driver of performance differences.

::: {.callout-note collapse="true" title="Welch t-test summary"}
```{python}
#| label: welch_ttest_avg

d = df_joined.dropna(subset=["average","access type"]).copy()

open_avg = d.loc[d["access type"]=="open-source","average"].astype(float)
prop_avg = d.loc[d["access type"]=="proprietary","average"].astype(float)

t, p = ttest_ind(open_avg, prop_avg, equal_var=False)

print(f"Welch's t-test result: t = {t:.2f}, p = {p:.2f}")
```
:::

::: {.callout-note collapse="true" title="ANCOVA summary"}
```{python}
#| label: ancova_avg
#| output: asis
#| code-fold: true

dd = df_joined.dropna(subset=["average","model_size","access type"]).copy()
dd["log_size"] = np.log10(dd["model_size"].astype(float))

m = smf.ols("average ~ log_size + C(Q('access type'))", data=dd).fit()
HTML(m.summary().tables[1].as_html())
```
:::

### Task-level benchmark comparison

To compare performance between open-source and proprietary models across evaluation tasks, we analyzed six benchmark metrics. Figure 3 visualizes score distributions for each benchmark using boxplots grouped by access type.

Except for `MUSR`, proprietary models show higher medians, particularly for `IFEVAL` and `MMLU-PRO`. However, overlapping interquartile ranges for other metrics indicate that some open-source models perform comparably. The distinct behavior of `MUSR` is not surprising, as it focuses on multimodal reasoning abilities that differ from the primarily text-based benchmarks.

```{python}
#| label: per_task_boxplots
#| fig-cap: "Figure 4: Task-level performance distributions by access type"
#| fig-width: 8
#| fig-height: 3.5

tasks = [c for c in ["ifeval","mmlu-pro","math","gpqa","bbh","musr"] if c in df_joined.columns]
n = len(tasks)

fig, axes = plt.subplots(1, n, figsize=(3.5*n, 3.5), constrained_layout=True)  
axes = axes.flatten()

palette = {"open-source": "#A1C9F4", "proprietary": "#8DE5A1"}

for i, col in enumerate(tasks):
    sns.boxplot(
        data=df_joined,
        x="access type",
        y=col,
        hue="access type",
        palette=palette,
        legend=False,
        ax=axes[i]
    )
    axes[i].set_xlabel("")
    axes[i].set_ylabel(f"{col} (%)")
    axes[i].set_title(col.upper())

for j in range(i+1, len(axes)):
    axes[j].axis("off")

plt.show()
```

## Conclusion

This project demonstrated a complete data science workflow-from data collection through API interaction and web scraping, to data cleaning, merging heterogeneous datasets, and statistical modeling. We developed skills in handling dynamic web content with `Selenium`, processing complex `JSON` structures, and applying appropriate `statistical methods` (correlation, t-tests, ANCOVA) to answer domain-specific research questions. The analysis reinforced the importance of normalizing metrics for fair comparison and controlling for confounding variables when drawing conclusions. In addition, we explored multiple `plotting libraries` to enhance our data visualization skills and deepen our understanding of effective visual communication.

A significant limitation is sample size. The [HuggingFace Open Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) displays 4,756 models as of November 2024, but we scraped only models retrieved from the API due to our method's inability to handle the scrollable table feature. This constraint affects our analysis in multiple ways: trends remain unclear (e.g., we cannot draw conclusions about models exceeding 20 billion parameters in Figure 1), statistical power is limited, and interpretations carry greater uncertainty. A more comprehensive scraping approach would enable collection of significantly more benchmark observations.

In future steps, improving the scraping algorithm to handle scrollable table features would enable collection of substantially more observations, supporting more thorough statistical analysis and clearer identification of trends across the full model size spectrum.